<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://feyza-droid.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://feyza-droid.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-26T22:40:29+01:00</updated><id>https://feyza-droid.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">baltic sea model</title><link href="https://feyza-droid.github.io/posts/2024/baltic-sea-model/" rel="alternate" type="text/html" title="baltic sea model"/><published>2024-12-26T00:00:00+01:00</published><updated>2024-12-26T00:00:00+01:00</updated><id>https://feyza-droid.github.io/posts/2024/baltic-sea-model</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2024/baltic-sea-model/"><![CDATA[<p>I am currently working on Baltic Sea modelling. I will share the details when I have more time.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/baltic_sea_model_convgnp_prediction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Baltic Sea Model" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> TO-DO: add ConvGNP model with better predictions </blockquote>]]></content><author><name></name></author><category term="research"/><category term="gaussian-processes"/><summary type="html"><![CDATA[Baltic Sea Model from Reanalysis Physics Data]]></summary></entry><entry><title type="html">designing machine learning systems</title><link href="https://feyza-droid.github.io/posts/2024/designing-machine-learning-systems/" rel="alternate" type="text/html" title="designing machine learning systems"/><published>2024-12-26T00:00:00+01:00</published><updated>2024-12-26T00:00:00+01:00</updated><id>https://feyza-droid.github.io/posts/2024/designing-machine-learning-systems</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2024/designing-machine-learning-systems/"><![CDATA[<p>I read Designing Machine Learning Systems occasionally and I would like to write what I learn from this book. Starting from the topic ‘Handling the Lack of Labels’.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/designing_machine_learning_systems.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Designing Machine Learning Systems" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> TO-DO: write Handling the Lack of Labels (page 94). Weak supervision, Semi-supervision, Transfer Learning, Active learning. </blockquote>]]></content><author><name></name></author><category term="learning"/><category term="ml-design"/><summary type="html"><![CDATA[Designing Machine Learning Systems]]></summary></entry><entry><title type="html">website to-dos</title><link href="https://feyza-droid.github.io/posts/2024/to-dos/" rel="alternate" type="text/html" title="website to-dos"/><published>2024-12-26T00:00:00+01:00</published><updated>2024-12-26T00:00:00+01:00</updated><id>https://feyza-droid.github.io/posts/2024/to-dos</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2024/to-dos/"><![CDATA[<p>This post is for myself to keep track of the todos related to my website.</p> <ul> <li>update publications tab</li> <li>update cv tab</li> <li>update books tab</li> <li>update projects tab</li> </ul>]]></content><author><name></name></author><category term="todo"/><category term="todo"/><summary type="html"><![CDATA[To-Dos]]></summary></entry><entry><title type="html">MapReduce</title><link href="https://feyza-droid.github.io/posts/2022/map-reduce/" rel="alternate" type="text/html" title="MapReduce"/><published>2022-11-01T00:00:00+01:00</published><updated>2022-11-01T00:00:00+01:00</updated><id>https://feyza-droid.github.io/posts/2022/map-reduce</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2022/map-reduce/"><![CDATA[<h2 id="distributed-file-systems">Distributed File Systems</h2> <p><br/></p> <h3 id="mining-of-massive-datasets">Mining of Massive Datasets</h3> <p><br/></p> <h3 id="map-reduce">Map-Reduce</h3> <ul> <li>Distributed File System</li> <li>Computational Model</li> <li>Scheduling and Data Flow</li> <li>Refinements</li> </ul> <p>What happens when the data is too big to fit in the memory? In classical data mining, you bring portion of the data, process it in batches and write back results to disk. With the 2016 technology, it takes 46+ days just to read 10 billion web pages. This is not feasible/unacceptable, therefore you need to parallize it by using multiple disks and CPUs. The main aim with cluster computing is to reduce the time spent.</p> <p><br/></p> <h3 id="cluster-architecture">Cluster Architecture</h3> <p>You have the racks consisting of commodity Linux nodes. You use them because they are very cheap. Each rack has 16 to 64 of these commodity Linux nodes. These nodes are connected by a switch. This switch in a rack is typically a gigabit switch so there is 1 Gbps bandwidth between any pair of nodes in rack. The racks themselves are connected by backbone switches. This backbone switches have higher bandwidths 2 to 10 Gbps between racks. You rack up multiple racks and you get a data center. This is the standard classical architecture for storing and mining very large datasets. In 2011, unofficial sources estimated that Google had 1 million nodes like this.</p> <p><br/></p> <h3 id="cluster-computing-challenges">Cluster Computing Challenges</h3> <p>1 - Node failures - A single server can stay up for 3 years (1000 years) - 1000 servers in cluster -&gt; 1 failure/day - 1M servers in cluser -&gt; 1000 failures/day</p> <ul> <li> <p>[Persistency] How to store data persistently and keep it available if nodes can fail? Persistence means that once you store the data, you’re guaranteed you can read it again. But if the node in which you stored the data fails, then you can’t read the data. You might even lose the data.</p> </li> <li> <p>[Availability] How to deal with node failures during a long-running computation? (Availability.) When you run half way through the computation, at this critical point, a couple of nodes fail. And that node had data that is necessary for the computation. In the worst case, you may have to go back and restart the computation all over again. But when you are running this computation the node again may fail. Therefore you need an infrastructure that can hide these kind of node failures and let the computation go to completion even if nodes fail.</p> </li> </ul> <p>2 - Network bottleneck - Network bandwidth = 1 Gbps - Moving 10TB takes approximately 1 day</p> <ul> <li>You need a framework that doesn’t move data around so much while it’s doing computation.</li> </ul> <p>3 - Distributed programming is hard! - Need a simple model that hides most of the complexity of distributed programming. And makes it easy to write algorithms that can mine very massive datasets. - Hard to write a program so that to avoid race conditions and various kind of complications.</p> <p><br/></p> <h3 id="map-reduce-1">Map-Reduce</h3> <ul> <li>Map reduce addresses the challenges of cluster computing <ul> <li>[To solve 1st problem about Persistency and Availability] Store data redundantly on multiple nodes for persistence and availability (Redundant Storage Infrastructure)</li> <li>[To solve 2nd problem about Network Bottleneck] Move computation close to data to minimize data movement</li> <li>[To solve 3rd problem about Distributed Programming is hard] Simple programming model to hide the complexity of all this magic</li> </ul> </li> </ul> <p><br/></p> <h3 id="redundant-storage-infrastructure">Redundant Storage Infrastructure</h3> <ul> <li>Distributed File System <ul> <li>It is a file system that stores data across a cluster, but stores each piece of data multiple times.</li> <li>It provides global file namespace, redundancy and availability</li> <li>There are multiple implementations of distributed file systems. E.g. Google GFS (Google file system); Hadoop HDFS (Hadoop distributed file system)</li> </ul> </li> <li>Typical usage pattern <ul> <li>There are some typical usage patterns that these distributed file systems are optimized for.</li> <li>Once the data is written, it is read very often. But when it is updated, it’s updated through appends.</li> <li>Typical usage pattern consists of writing the data once, reading it multiple times and appending to it occasionally.</li> <li>Huge files (100s of GB to TB)</li> <li>Data is rarely updated in place</li> <li>Reads and appends are common</li> </ul> </li> <li>Data kept in “chunks” spread across machines (Example, one file is divided into 6 chunks and kept in different servers)</li> <li>Each chunk replicated on different machines (ensures persistence and availability). Replicas of the chunk are never on the same chunk server. They are always on different chunk servers.</li> <li> <p>[Bring computation to data!] Chunk servers also serve as compute servers. Whenever your computation has to access data, that computation is actually scheduled on the chunk server that actually contains the data. This way you avoid moving data to where the computation needs to run, but instead you move the computation to where the data is. That is how you avoid unnecessary data movement.</p> </li> <li>Chunk servers <ul> <li>File is split into contiguous chunks (16-64 MB)</li> <li>Each chunk replicated (usually 2x or 3x). 3x replication is the most common</li> <li>Try to keep replicas in different racks (when the switch on a rack fails, the entire rack becomes inaccessible)</li> </ul> </li> <li>Master node <ul> <li>a.k.a. Name node in Hadoop’s HDFS</li> <li>Stores metadata about where files are stored (For example, it’ll know that file one is divided into 6 chunks and locations of each of the 6 chunks and the locations of the replicas)</li> <li>Might be replicated (Otherwise it will become a single point of failure)</li> </ul> </li> <li>Client library for file access <ul> <li>When a client or an algorithm that needs to access the data tries to access a file it goes through the client library.</li> <li>Talks to master to find chunk servers that actually store the chunks</li> <li>After talking to master and learning the chunk servers location, client directly connects to chunk servers to access data</li> <li>So the data access actually happens in peer-to-peer fashion without going through the master node</li> </ul> </li> </ul> <p><br/></p> <hr/> <h2 id="the-mapreduce-computational-model">The MapReduce Computational Model</h2> <h3 id="programming-model-mapreduce">Programming Model: MapReduce</h3> <p><br/></p> <h3 id="warm-up-task">Warm-up Task:</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- We have a huge text document
- Count the number of times each distinct word appears in the file
- Sample applications
    - Analyze web server logs to find popular URLs
    - Term statistics for search engine
</code></pre></div></div> <ul> <li>Case 1: <ul> <li>File too large for memory, but all &lt;word, count&gt; pairs fit in memory</li> <li>Simple approach by using HashTable would work (word (key) -&gt; count) to keep track of the count of each unique word</li> </ul> </li> <li>Case 2: <ul> <li>Even the &lt;word, count&gt; pairs don’t fit in memory</li> <li> <table> <tbody> <tr> <td>words(doc.txt)</td> <td>sort</td> <td>uniq -c (where words takes a file and outputs the words in it, one per a line, uniq -c counts the occurrences of the same word)</td> </tr> </tbody> </table> </li> <li>case 2 captures the essence of MapReduce (great thing is that it is naturally parallelizable)</li> </ul> </li> <li>Map [words(doc.txt)] <ul> <li>Scan input file record-at-a-time</li> <li>Extract something you care about from each record (keys), in this case it is words</li> </ul> </li> <li>Group by key [sort] <ul> <li>Sort and shuffle</li> </ul> </li> <li>Reduce [uniq -c] <ul> <li>Aggregate, summarize, filter or transform</li> <li>Write the result</li> </ul> </li> </ul> <p>Outline stays the same, Map and Reduce change to fit the problem</p> <p><br/></p> <h3 id="more-formally">More formally</h3> <ul> <li>Input: a set of key-value pairs</li> <li>Programmer specifies two methods: <ul> <li>1st METHOD - Map(k, v) -&gt; &lt;k’, v’&gt; <ul> <li>Takes a key-value pair and outputs a set of key-value pairs</li> <li>There is one Map call for every (k,v) pair</li> </ul> </li> <li>2nd METHOD - Reduce (k’, &lt;v’&gt;) -&gt; &lt;k’, v’’&gt; <ul> <li>All values v’ with same key k’ are reduced together</li> <li>There is one Reduce function call per unique key k’</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <p>MapReduce system is built around doing only sequential reads of files and never random accesses there it is more efficient.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/map_reduce_word_counting.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Map Reduce Word Counting" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>map(key, value): # key: document name; value: text of the document
    for each word w in value: # it scans the input document, value here is the input document
        emit(w, 1) # output is the set of intermediate key-value pairs, for each word in the input document it emits that word and the number 1, it is a tuple whose key is the word and whose value is the number 1
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>reduce(key, values): # key: a word; value: an iterator over counts
    result = 0
    for each count v in values:
        result += v # sums all of the values (in this case sums all 1s to count the occurrences of the unique word)
    emit(key, result) # key is the word and value is the sum for this example
</code></pre></div></div> <p><br/></p> <h3 id="example-host-size">Example: Host size</h3> <ul> <li>Suppose we have a large web corpus with a metadata file formatted as follows: <ul> <li>Each record of the form: (URL, size, date, …)</li> </ul> </li> <li> <p>For each host, find the total number of bytes</p> </li> <li>Map <ul> <li>For each record, output (hostname(URL), size)</li> </ul> </li> <li>Reduce <ul> <li>Sum the sizes for each host</li> </ul> </li> </ul> <p><br/></p> <h3 id="example-language-model">Example: Language Model</h3> <ul> <li> <p>Count number of times each 5-word sequence occurs in a large corpus of documents</p> </li> <li>Map <ul> <li>Extract (5-word sequence, count) from document</li> </ul> </li> <li>Reduce <ul> <li>Combine the counts</li> </ul> </li> </ul> <hr/> <h2 id="scheduling-and-data-flow">Scheduling and Data Flow</h2> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/map_reduce_a_diagram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Map Reduce A Diagram" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/posts/map_reduce_in_parallel.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Map Reduce In Parallel" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="map-reduce-environment">Map-Reduce: Environment</h3> <ul> <li>Map-Reduce environment takes care of: <ul> <li>Partitioning the input data</li> <li>Scheduling the program’s execution across a set of machines</li> <li>Performing the group by key step</li> <li>Handling node failures</li> <li>Managing required inter-machine communication</li> </ul> </li> </ul> <p><br/></p> <h3 id="data-flow">Data Flow</h3> <ul> <li>Input and final output are stored on the distributed file system (DFS): <ul> <li>Scheduler tries to schedule map tasks “close” to physical storage location of input data (The Map-Reduce system try to schedule each map task on a chunk server that holds a copy of the corresponding chunk. So there is no actual copy, a data copy associated with the map step of the Map-Reduce program.)</li> </ul> </li> <li>Intermediate results are stored on local file system (FS) of Map and Reduce workers (Why are such debated results not stored in the distributed file system? Because there is some overhead to storing data in the distributed file system. To avoid more network traffic.)</li> <li>Output is often input to another MapReduce task</li> </ul> <p><br/></p> <h3 id="coordination-master">Coordination: Master</h3> <ul> <li>Master node takes care of coordination: <ul> <li>Task status: (idle, in-progress, completed)</li> <li>Idle tasks get scheduled as workers become available</li> <li>When a map task completes, it sends the master the location and sizes of its R intermediate files (on local file system), one for each reducer</li> <li>Master pushes this info to reducers</li> <li>Once the reducers know that all the mappers map tasks are completed, then they copy the intermediate file from each of the map tasks. And then they can proceed with their work.</li> </ul> </li> <li>Master pings workers periodically to detect failures</li> </ul> <p><br/></p> <h3 id="dealing-with-failures">Dealing with failures</h3> <ul> <li>Map worker failure <ul> <li>Map tasks are completed or in-progress at worker are reset to idle (The reason why we are resetting the completed tasks is that since the intermediate results are written to local file system of the map worker, intermediate results might be lost if the map worker fails. If the map worker fails, then the node fails. Then all the intermediate output created by all the map tasks that have ran on that worker, are lost.)</li> <li>Idle tasks eventually rescheduled on other worker(s)</li> </ul> </li> <li>Reduce worker failure <ul> <li>Only in-progress tasks are reset to idle (The reason why we aren’t resetting the completed tasks is that since the output is written to distributed file system, the output is not lost even if the reduce worker fails. While completed tasks don’t need to be redone.)</li> <li>Idle Reduce tasks restarted on other workers(s)</li> </ul> </li> <li>Master failure <ul> <li>MapReduce task is aborted and client is notified</li> </ul> </li> </ul> <p><br/></p> <h3 id="how-many-map-and-reduce-jobs">How many Map and Reduce jobs?</h3> <ul> <li>M map tasks, R reduce tasks</li> <li>Rule of thumb: <ul> <li>Make M much larger than the number of nodes in the cluster</li> <li>One DFS chunk per map is common (Rule of thumb is to have on map task per DFS chunk. The reason is simple. Imagine that there is one map task per node in the cluster and during processing the node fails. If a node fails then that map task needs to be rescheduled on another node in the cluster when it becomes available. Since all the other nodes are processing, one of those nodes has to complete before this map task can be scheduled on that node and so, the entire computation is slowed down by the time it takes to complete this map task to redo the failed map task. So instead of one map task on a given node, there are many small map tasks on a given node, and that node fails, then those map tasks can then be spread across all the available nodes and so the entire task will complete much faster.)</li> <li>Improves dynamic load balancing and speeds up recovery from worker failures</li> </ul> </li> <li>Usually R is smaller than M (and usually it is even smaller than the total number of nodes in the system) <ul> <li>Because output is spread across R files (It is usually convenient to have the output spread across a small number of nodes rather than across a large number of nodes)</li> </ul> </li> </ul> <hr/> <h2 id="notes">NOTES</h2> <ul> <li>This is my summary of the videos below.</li> <li>Be aware of that videos are recorded in 2016. Some information may be outdated.</li> </ul> <hr/> <h2 id="references">References</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- https://www.youtube.com/watch?v=jDlrfBLAIuQ
- https://www.youtube.com/watch?v=VEh91xTIEIU
- https://www.youtube.com/watch?v=4_Eco-v4wlI
</code></pre></div></div>]]></content><author><name></name></author><category term="coding"/><category term="software-engineering"/><summary type="html"><![CDATA[MapReduce]]></summary></entry><entry><title type="html">dynamic programming</title><link href="https://feyza-droid.github.io/posts/2022/dynamic-programming/" rel="alternate" type="text/html" title="dynamic programming"/><published>2022-10-22T00:00:00+02:00</published><updated>2022-10-22T00:00:00+02:00</updated><id>https://feyza-droid.github.io/posts/2022/dynamic-programming</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2022/dynamic-programming/"><![CDATA[<p>I am currently practicing Dynamic Programming problems taught by freeCodeCamp, one of my favorite resources on the internet. <a href="https://www.youtube.com/watch?v=oBt53YbR9Kk">freeCodeCamp Dynamic Programming - Learn to Solve Algorithmic Problems &amp; Coding Challenges</a></p> <p><a href="https://github.com/feyza-droid/dynamic_programming">Python Solutions in GitHub</a></p> <hr/> <h2 id="dp-problems-list">DP Problems List</h2> <ul> <li>canSum ➡️ Can you do it? yes/no (Decision Problem) </li> <li>howSum ➡️ How will you do it? (Combinatoric Problem) </li> <li>bestSum ➡️ What is the best way to do it? (Optimization Problem) </li> <li>canConstruct ➡️ Can you do it? (with strings) </li> <li>countConstruct ➡️ How many different ways to do it? (with strings)</li> <li>allConstruct ➡️ What are the ways to do it? (with strings) </li> </ul> <h2 id="how-to-solve-with-recursion--memoization">How to solve with recursion &amp; memoization?</h2> <ol> <li>Make it work <ul> <li>visualize the problem as a tree</li> <li>implement the tree using recursion</li> <li>test it</li> </ul> </li> <li>Make it efficient <ul> <li>add a memo object</li> <li>add a base case to return memo values</li> <li>store return values into the memo</li> </ul> </li> </ol> <h2 id="how-to-solve-with-tabulation">How to solve with tabulation?</h2> <ul> <li>visualize the problem as a table</li> <li>size the table based on the inputs</li> <li>initialize the table with default values</li> <li>seed the trivial answer into the table</li> <li>iterate through the table</li> <li>fill further positions based on the current position</li> </ul> <blockquote> TO-DO: Reexamine the space and time complexity of each solution. </blockquote> <blockquote> TO-DO: add code pieces here </blockquote> <blockquote> TO-DO: My understanding and summary </blockquote>]]></content><author><name></name></author><category term="coding"/><category term="dynamic-programming"/><category term="data-structures"/><category term="algorithms"/><summary type="html"><![CDATA[freeCodeCamp Dynamic Programming - Learn to Solve Algorithmic Problems & Coding Challenges]]></summary></entry><entry><title type="html">data efficient offline reinforcement learning literature survey</title><link href="https://feyza-droid.github.io/posts/2022/data-efficient-offline-rl-literature-survey/" rel="alternate" type="text/html" title="data efficient offline reinforcement learning literature survey"/><published>2022-10-21T00:00:00+02:00</published><updated>2022-10-21T00:00:00+02:00</updated><id>https://feyza-droid.github.io/posts/2022/data-efficient-offline-rl-literature-survey</id><content type="html" xml:base="https://feyza-droid.github.io/posts/2022/data-efficient-offline-rl-literature-survey/"><![CDATA[<h4 id="paper-list">PAPER LIST</h4> <ul> <li><a href="https://">Deep Learning on a Data Diet: Finding Important Examples Early in Training</a></li> <li><a href="https://">Dataset Pruning: Reducing Training Data by Examining Generalization Influence</a></li> <li><a href="https://">Impact of Data Pruning on Machine Learning Algorithm Performance</a></li> <li><a href="https://">Data-Efficient Hierarchical Reinforcement Learning</a></li> <li><a href="https://">Beyond neural scaling laws beating power law scaling via data pruning</a></li> <li><a href="https://">Data pruning</a></li> <li><a href="https://">Don't Change the Algorithm, Change the Data Exploratory Data for Offline Reinforcement Learning</a></li> <li><a href="https://">Prioritized Experience Replay</a></li> <li><a href="https://">What Matters in Learning from Offline Human Demonstrations for Robot Manipulation</a></li> <li><a href="https://">Estimating Training Data Influence by Tracing Gradient Descent</a></li> <li><a href="https://">PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators</a></li> <li><a href="https://">Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents</a></li> <li><a href="https://">Conservative Q-Learning for Offline Reinforcement Learning</a></li> <li><a href="https://">D4RL: datasets for deep data-driven reinforcement learning</a></li> <li><a href="https://">A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</a></li> <li><a href="https://">DeepMind Control Suite</a></li> <li><a href="https://">Improving DDPG via Prioritized Experience Replay</a></li> <li><a href="https://">Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning</a></li> <li><a href="https://">Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings</a></li> <li><a href="https://">Multi-Task Offline Reinforcement Learning with Conservative Data Sharing</a></li> <li><a href="https://">Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems</a></li> <li><a href="https://">Offline RL: Approaching Reinforcement Learning in a data-driven manner</a></li> <li><a href="https://">RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning</a></li> <li><a href="https://">Single-Shot Pruning for Offline Reinforcement Learning</a></li> <li><a href="https://">Diversity is all you need: Learning skills without a reward function</a></li> <li><a href="https://">URLB: unsupervised reinforcement learning benchmark</a></li> <li><a href="https://">Aps: Active pretraining with successor feature</a></li> <li><a href="https://">Reinforcement learning with prototypical representations</a></li> <li><a href="https://">Hindsight Experience Replay</a></li> <li><a href="https://">Data-Efficient Pipeline for Offline Reinforcement Learning with Limited Data</a></li> <li><a href="https://">Representation Matters: Offline Pretraining for Sequential Decision Making</a></li> <li><a href="https://">Pretraining Representations for Data-Efficient Reinforcement Learning</a></li> <li><a href="https://">Dataset Pruning for Resource-constrained Spoofed Audio Detection</a></li> <li><a href="https://">Conservative Data Sharing for Multi-Task Offline Reinforcement Learning</a></li> <li><a href="https://">Data Valuation for Offline Reinforcement Learning</a></li> <li><a href="https://">Can Wikipedia Help Offline Reinforcement Learning?</a></li> <li><a href="https://">Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning</a></li> <li><a href="https://">DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning</a></li> <li><a href="https://">OFFLINE REINFORCEMENT LEARNING HANDS-ON</a></li> </ul> <hr/> <h4 id="to-do-list">TO-DO LIST</h4> <ul> <li>organize papers (eliminate them by reading their abstracts and add new papers by searching for the most relavant and up-to-date literature on this topic</li> <li>summarize them with the first pass (abstract, overall idea and figures)</li> <li>add link to each paper</li> </ul> <blockquote> TO-DO: My understanding of each paper </blockquote> <hr/>]]></content><author><name></name></author><category term="research"/><category term="reinforcement-learning"/><category term="offline-reinforcement-learning"/><summary type="html"><![CDATA[data efficient offline reinforcement learning literature survey]]></summary></entry></feed>